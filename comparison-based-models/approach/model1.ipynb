{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJwrk1sNe3mB",
        "colab_type": "text"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvq2etNOe3DC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r ../../requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAyMn10qe97h",
        "colab_type": "text"
      },
      "source": [
        "Read data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw_Gsw5fwbQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "data_folder = '../../datasets/comparison-based-dataset/'\n",
        "\n",
        "scores_path = data_folder + 'website_scores.csv'\n",
        "images_path = data_folder + 'images/'\n",
        "\n",
        "def get_scores():\n",
        "\n",
        "    images = []\n",
        "    scores = []\n",
        "\n",
        "    with open(scores_path) as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "        line_count = 0\n",
        "\n",
        "        for row in csv_reader:\n",
        "            if line_count == 0:\n",
        "\n",
        "                line_count += 1\n",
        "            else:\n",
        "\n",
        "                scores.append(float(row[1]))\n",
        "                line_count += 1\n",
        "                image_name = row[0]\n",
        "                image_name = image_name[5:]\n",
        "\n",
        "                images.append(images_path + image_name + '.png')\n",
        "\n",
        "\n",
        "    return (images, scores)\n",
        "\n",
        "\n",
        "train_images, scores = get_scores()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJQoyuiJi_fc",
        "colab_type": "text"
      },
      "source": [
        "Shuffle the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HCW2roAw2iB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "# np.random.seed(2000)\n",
        "\n",
        "temp = list(zip(train_images, scores))\n",
        "random.shuffle(temp)\n",
        "\n",
        "train_images, scores = zip(*temp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzsY8pNLjP_8",
        "colab_type": "text"
      },
      "source": [
        "Display the first 3 images to make sure everything is ok. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWrGufbVw4mQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.image as mping\n",
        "for ima in train_images[0:3]:\n",
        "    img = mping.imread(ima)\n",
        "    imgplot = plt.imshow(img)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-Ch28N9jYmL",
        "colab_type": "text"
      },
      "source": [
        "Read and reshape images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmGGT096x3i-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np \n",
        "\n",
        "width = 256 \n",
        "height = 192 \n",
        "channels = 3\n",
        "\n",
        "def read_and_process_images(list_of_images):\n",
        "  X = []\n",
        "  \n",
        "  for image in list_of_images:\n",
        "    X.append(cv2.resize(cv2.imread(image, cv2.IMREAD_COLOR), (width, height), \n",
        "                        interpolation=cv2.INTER_AREA))\n",
        "  return X\n",
        "\n",
        "\n",
        "X = np.array(read_and_process_images(train_images))\n",
        "y = np.array(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZDo_Fq1jdRw",
        "colab_type": "text"
      },
      "source": [
        "Display shapes to check everything is ok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZTVnEUnzmdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = len(X)\n",
        "\n",
        "print('Length of X is: ', n)\n",
        "print('Shape of X is: ', X.shape)\n",
        "print('Shape of y is: ', y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhzrBn6vjkBR",
        "colab_type": "text"
      },
      "source": [
        "Display the first 2 images to make sure everything is ok. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N338g42bzSOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(25,15))\n",
        "num = 3\n",
        "\n",
        "for i in range(num):\n",
        "  plt.subplot(num, 1, i+1)\n",
        "  plt.imshow(X[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0_l2RJ2j0Fs",
        "colab_type": "text"
      },
      "source": [
        "The architecture that will be used for the CNN is AlexNet. First, we define the LRN (Local Response Normalization) layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKpXSzGR0D5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras import initializers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.layers.core import Layer\n",
        "from keras import backend as K\n",
        "\n",
        "class LRN(Layer):\n",
        "    \n",
        "    def __init__(self, n=5, alpha=0.0001, beta=0.75, k=2, **kwargs):\n",
        "        self.n = n\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.k = k\n",
        "        super(LRN, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.shape = input_shape\n",
        "        super(LRN, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        if K.common.image_dim_ordering == \"th\":\n",
        "            _, f, r, c = self.shape\n",
        "        else:\n",
        "            _, r, c, f = self.shape\n",
        "        half_n = self.n // 2\n",
        "        squared = K.square(x)\n",
        "        pooled = K.pool2d(squared, (half_n, half_n), strides=(1, 1),\n",
        "                         padding=\"same\", pool_mode=\"avg\")\n",
        "        if K.common.image_dim_ordering == \"th\":\n",
        "            summed = K.sum(pooled, axis=1, keepdims=True)\n",
        "            averaged = (self.alpha / self.n) * K.repeat_elements(summed, f, axis=1)\n",
        "        else:\n",
        "            summed = K.sum(pooled, axis=3, keepdims=True)\n",
        "            averaged = (self.alpha / self.n) * K.repeat_elements(summed, f, axis=3)\n",
        "        denom = K.pow(self.k + averaged, self.beta)\n",
        "        return x / denom\n",
        "    \n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return input_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-t_CJXdj7GX",
        "colab_type": "text"
      },
      "source": [
        "Define the loss function (Euclidean Distance Loss) and the metric (RMSE) that will be used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rveke7hu06-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        " \n",
        "def rmse(y_true, y_pred):\n",
        "\treturn K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
        "\n",
        "def euclidean_distance_loss(y_true, y_pred):\n",
        "  return 0.5 * K.mean(K.square(y_pred - y_true), axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5UQZuj_j_RB",
        "colab_type": "text"
      },
      "source": [
        "Define a function that calculates Pearson correlation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5hYD6wyDAgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import stats\n",
        "\n",
        "def pearsonr_ci(x,y,alpha=0.05):\n",
        "    ''' calculate Pearson correlation along with the confidence interval using scipy and numpy\n",
        "    Parameters\n",
        "    ----------\n",
        "    x, y : iterable object such as a list or np.array\n",
        "      Input for correlation calculation\n",
        "    alpha : float\n",
        "      Significance level. 0.05 by default\n",
        "    Returns\n",
        "    -------\n",
        "    r : float\n",
        "      Pearson's correlation coefficient\n",
        "    pval : float\n",
        "      The corresponding p value\n",
        "    lo, hi : float\n",
        "      The lower and upper bound of confidence intervals\n",
        "    '''\n",
        "    N = len(x)\n",
        "    r, p = stats.pearsonr(x,y)\n",
        "    r_z = np.arctanh(r)\n",
        "    se = 1/np.sqrt(N-3)\n",
        "    z = stats.norm.ppf(1-alpha/2)\n",
        "    lo_z, hi_z = r_z-z*se, r_z+z*se\n",
        "    lo, hi = np.tanh((lo_z, hi_z))\n",
        "    return r, p, lo, hi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmVKlmJtnNUl",
        "colab_type": "text"
      },
      "source": [
        "Construct the CNN. The first 5 convolutional layers of the network are initialized with weights of the pretrained model on the [Rating-Based Web Aesthetics dataset](https://github.com/calista-ai/website-aesthetics-datasets). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vt37Lxv0bR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = 0.001 # weight decay\n",
        "epochs = 100\n",
        "decay = 1e-4 \n",
        "base_lr = 0.001\n",
        "input_shape = X[0].shape\n",
        "batch_size = 10\n",
        "pretrained_model_filename = '../../pretrained-models/calista_rating_based.h5'\n",
        "\n",
        "def create_model(transfer_learning = False):\n",
        "  im_data = layers.Input(shape=input_shape, dtype='float32', name='im_data')\n",
        "\n",
        "  conv1 = layers.Conv2D(96, kernel_size=(11, 11), strides=(4, 4), name='conv1', \n",
        "                  activation='relu', input_shape=input_shape, \n",
        "                  kernel_regularizer=regularizers.l2(l))(im_data)\n",
        "\n",
        "  pool1 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
        "  norm1 = LRN(name=\"norm1\")(pool1)\n",
        "\n",
        "  layer1_1 = layers.Lambda(lambda x: x[:, :, :, :48])(norm1)\n",
        "  layer1_2 = layers.Lambda(lambda x: x[:, :, :, 48:])(norm1)\n",
        "\n",
        "  conv2_1 = layers.Conv2D(128, kernel_size=(5, 5), strides=(1, 1),\n",
        "                  activation='relu',\n",
        "                  padding='same', \n",
        "                  name='conv2_1', \n",
        "                  kernel_regularizer=regularizers.l2(l))(layer1_1)\n",
        "\n",
        "  conv2_2 = layers.Conv2D(128, kernel_size=(5, 5), strides=(1, 1),\n",
        "                  activation='relu',\n",
        "                  padding='same', \n",
        "                  name='conv2_2',\n",
        "                  kernel_regularizer=regularizers.l2(l))(layer1_2)\n",
        "\n",
        "  conv2 = layers.Concatenate(name='conv_2')([conv2_1, conv2_2])\n",
        "\n",
        "  pool2 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv2)\n",
        "  norm2 = LRN(name=\"norm2\")(pool2)\n",
        "\n",
        "  conv3 = layers.Conv2D(384, kernel_size=(3, 3), strides=(1, 1), activation='relu', \n",
        "                  name='conv3',\n",
        "                  padding='same',\n",
        "                  kernel_regularizer=regularizers.l2(l))(norm2)\n",
        "\n",
        "  layer3_1 = layers.Lambda(lambda x: x[:, :, :, :192])(conv3)\n",
        "  layer3_2 = layers.Lambda(lambda x: x[:, :, :, 192:])(conv3)\n",
        "\n",
        "  conv4_1 = layers.Conv2D(192, kernel_size=(3, 3), strides=(1, 1),\n",
        "                  activation='relu', \n",
        "                  padding='same',\n",
        "                  name='conv4_1',\n",
        "                  kernel_regularizer=regularizers.l2(l))(layer3_1)\n",
        "\n",
        "  conv4_2 = layers.Conv2D(192, kernel_size=(3, 3), strides=(1, 1),\n",
        "                  activation='relu', \n",
        "                  padding='same',\n",
        "                  name='conv4_2',\n",
        "                  kernel_regularizer=regularizers.l2(l))(layer3_2)\n",
        "\n",
        "  conv4 = layers.Concatenate(name='conv_4')([conv4_1, conv4_2])\n",
        "\n",
        "  layer4_1 = layers.Lambda(lambda x: x[:, :, :, :192])(conv4)\n",
        "  layer4_2 = layers.Lambda(lambda x: x[:, :, :, 192:])(conv4)\n",
        "\n",
        "  conv5_1 = layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1),\n",
        "                  activation='relu',\n",
        "                  padding='same', \n",
        "                  name='conv5_1',\n",
        "                  kernel_regularizer=regularizers.l2(l))(layer4_1)\n",
        "\n",
        "  conv5_2 = layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1),\n",
        "                  activation='relu',\n",
        "                  padding='same', \n",
        "                  name='conv5_2',\n",
        "                  kernel_regularizer=regularizers.l2(l))(layer4_2)\n",
        "\n",
        "  conv5 = layers.Concatenate(name='conv_5')([conv5_1, conv5_2])\n",
        "\n",
        "  pool5 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv5)\n",
        "\n",
        "  flat = layers.Flatten()(pool5)\n",
        "  fc6 = layers.Dense(1024, activation='relu', name='fc6', \n",
        "                  kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
        "                  bias_initializer='zeros',\n",
        "                  kernel_regularizer=regularizers.l2(l))(flat)\n",
        "  drop6 = layers.Dropout(0.5)(fc6)\n",
        "\n",
        "  fc7 = layers.Dense(512, activation='relu', name='fc7', \n",
        "                  kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
        "                  bias_initializer='zeros',\n",
        "                  kernel_regularizer=regularizers.l2(l))(drop6)\n",
        "  drop7 = layers.Dropout(0.5)(fc7)\n",
        "\n",
        "  fc8 = layers.Dense(1, name='fc8', \n",
        "                  kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
        "                  bias_initializer='zeros')(drop7)\n",
        "\n",
        "  model = models.Model(inputs=im_data, outputs=fc8)\n",
        "\n",
        "  if (transfer_learning == True):\n",
        "    model.load_weights(pretrained_model_filename, by_name = True)\n",
        "    print('Transfer learning completed')\n",
        "\n",
        "  sgd = optimizers.SGD(lr=base_lr, momentum=0.9, decay=decay, nesterov=True)\n",
        "  model.compile(loss=euclidean_distance_loss , optimizer=sgd, metrics=[rmse])\n",
        "\n",
        "  return model \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQCPLu7zp7dI",
        "colab_type": "text"
      },
      "source": [
        "Create a file to save model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCg-fbvOCeOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "import gc\n",
        "\n",
        "date_time = datetime.now().strftime(\"%m_%d_%Y__%H_%M_%S\")\n",
        "output_filename = 'results_' + date_time + '.csv'\n",
        "\n",
        "with open(output_filename, \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"images\", \"gt_scores\", \"predictions\"])\n",
        "\n",
        "predictions = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Kf5A-4pXAE",
        "colab_type": "text"
      },
      "source": [
        "Use Leave-one-out cross validation (LOOCV) to evalute the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3ajwkdmENp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_start = 0\n",
        "k = n\n",
        "\n",
        "for i in range(k_start, k):\n",
        "  print('**** PROGRESS: %d/%d ****' % (i, k))\n",
        "  if i == 0:\n",
        "    X_train = X[(i+1):]\n",
        "    y_train = y[(i+1):]\n",
        "  elif i == n-1:\n",
        "    X_train = X[:i]\n",
        "    y_train = y[:i]\n",
        "  else:\n",
        "    X_train = np.concatenate((X[:i], X[(i+1):]), axis = 0)\n",
        "    y_train = np.concatenate((y[:i], y[(i+1):]), axis = 0)\n",
        "\n",
        "  X_val = X[i]\n",
        "  X_val = X_val.reshape(1, height, width, channels)\n",
        "  y_val = y[i]\n",
        "\n",
        "  ntrain = len(X_train)\n",
        "  # check shapes\n",
        "  # print('X_train length : ' + str(ntrain))\n",
        "  # print('X_train : ' + str(X_train.shape))\n",
        "  # print('y_train : ' + str(y_train.shape))\n",
        "  # print('X_val : ' + str(X_val.shape))\n",
        "\n",
        "  train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "  val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "  train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "  model = create_model(transfer_learning=True)\n",
        "\n",
        "  history = model.fit_generator(train_generator, \n",
        "                                steps_per_epoch = ntrain // batch_size,\n",
        "                                epochs = epochs)\n",
        "\n",
        "  val_data = val_datagen.flow(X_val, batch_size=1).next()\n",
        "\n",
        "  pred = float(model.predict(val_data))\n",
        "  predictions.append(pred)\n",
        "\n",
        "  # save output \n",
        "  new_row = [train_images[i], scores[i], pred]\n",
        "  with open(output_filename, \"a\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(new_row)\n",
        "\n",
        "  del X_train, X_val, y_train, y_val, model\n",
        "  gc.collect()\n",
        "\n",
        "print('********* DONE *********')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAgf1EkfqIVi",
        "colab_type": "text"
      },
      "source": [
        "Create a scatterplot to check the relationship between ground truth and predicted scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g1-nez8HC9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.polynomial.polynomial import polyfit\n",
        "\n",
        "y_val = np.array(scores[0:k])\n",
        "predictions = np.array(predictions)\n",
        "\n",
        "b, m = polyfit(y_val, predictions, 1)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.scatter(y_val, predictions, c='c')\n",
        "plt.plot(y_val, b + m * y_val, '-', c='b')\n",
        "plt.xlabel('User ratings')\n",
        "plt.ylabel('Predicted ratings')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANlilLu8qofJ",
        "colab_type": "text"
      },
      "source": [
        "Calculate the Pearson correlation and the RMSE between ground truth and predicted scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blGWYX4pHD1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "corr, p, lo, hi = pearsonr_ci(y_val, predictions)\n",
        "print('Pearsons correlation: r=%.2f, p=%.2e, CI=[%.2f, %.2f]' % (corr, p, lo, hi))\n",
        "rmse_test = sqrt(mean_squared_error(y_val, predictions))\n",
        "print('RMSE: %.3f' % rmse_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pimv6_Z_q5LX",
        "colab_type": "text"
      },
      "source": [
        "Plot the distribution of ground truth scores and the distribution of predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqISKJ6WHJRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "fig = plt.figure()\n",
        "sns.set(color_codes=True)\n",
        "\n",
        "bins = np.linspace(1, 9, num=15)\n",
        "\n",
        "sns.distplot(y_val, bins=bins, label='User ratings')\n",
        "\n",
        "sns.distplot(predictions, bins=bins, label='Predicted ratings')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}