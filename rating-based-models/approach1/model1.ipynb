{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEf9JrLz5_CF",
        "colab_type": "text"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkQV_zVl6BzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r ../../requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efYF6_WMXah-",
        "colab_type": "text"
      },
      "source": [
        "Import training and testing data.\n",
        "\n",
        "* {train, test}_images: contains the path of each image\n",
        "* {train, test}_scores: contains the user ratings of each image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZMovSJd0uxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "data_folder = '../../datasets/rating-based-dataset/preprocess/'\n",
        "\n",
        "train_data_path = data_folder + 'train_means_list.csv'\n",
        "test_data_path = data_folder + 'test_list.csv'\n",
        "images_path = data_folder + 'resized'\n",
        "\n",
        "def get_scores(scores_path):\n",
        "\n",
        "    images = []\n",
        "    scores = []\n",
        "\n",
        "    with open(scores_path) as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "        line_count = 0\n",
        "\n",
        "        for row in csv_reader:\n",
        "            if line_count == 0:\n",
        "\n",
        "                line_count += 1\n",
        "            else:\n",
        "\n",
        "                scores.append(float(row[1]))\n",
        "                line_count += 1\n",
        "                image_name = row[0]\n",
        "\n",
        "                images.append(images_path + image_name)\n",
        "\n",
        "    return (images, scores)\n",
        "\n",
        "train_images, train_scores = get_scores(train_data_path)\n",
        "test_images, test_scores = get_scores(test_data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tEbYZO-Xiom",
        "colab_type": "text"
      },
      "source": [
        "Shuffle the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKJHSKnC1b2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# np.random.seed(2000)\n",
        "\n",
        "temp = list(zip(train_images, train_scores))\n",
        "random.shuffle(temp)\n",
        "\n",
        "train_images, train_scores = zip(*temp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnBnXkoiXvhZ",
        "colab_type": "text"
      },
      "source": [
        "Display the first 3 images to make sure everything is ok. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRogEXdlr4bM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.image as mping\n",
        "for ima in train_images[0:3]:\n",
        "  img = mping.imread(ima)\n",
        "  imgplot = plt.imshow(img)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG6D1-2_YKwT",
        "colab_type": "text"
      },
      "source": [
        "Read the images as numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoVPwg2OsUFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "width = 256 \n",
        "height = 192 \n",
        "channels = 3\n",
        "\n",
        "def read_and_process_images(list_of_images):\n",
        "  X = []\n",
        "  \n",
        "  for image in list_of_images:\n",
        "\n",
        "    # images are already resized\n",
        "    # X.append(cv2.resize(cv2.imread(image, cv2.IMREAD_COLOR), (width, height), \n",
        "    #                     interpolation=cv2.INTER_AREA))\n",
        "\n",
        "    X.append(cv2.imread(image, cv2.IMREAD_COLOR))\n",
        "  \n",
        "  return X\n",
        "\n",
        "\n",
        "X_train = np.array(read_and_process_images(train_images))\n",
        "y_train = np.array(train_scores)\n",
        "\n",
        "X_val = np.array(read_and_process_images(test_images))\n",
        "y_val = np.array(test_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VumDJtpUYauI",
        "colab_type": "text"
      },
      "source": [
        "Display the first 3 images to make sure everything is ok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxRbpujAuE7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(25,15))\n",
        "columns = 3\n",
        "\n",
        "for i in range(columns):\n",
        "  plt.subplot(columns, 1, i+1)\n",
        "  plt.imshow(X_train[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B33pIO_a094",
        "colab_type": "text"
      },
      "source": [
        "Display shapes to check everything is ok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsI7encBa60J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntrain = len(X_train)\n",
        "nval = len(X_val)\n",
        "\n",
        "print('Shape of X_train is: ', X_train.shape)\n",
        "print('Shape of X_val is: ', X_val.shape)\n",
        "print('Shape of y_train is: ', y_train.shape)\n",
        "print('Shape of y_val is: ', y_val.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ObdOOkbbktT",
        "colab_type": "text"
      },
      "source": [
        "Get the weights of the pretrained model. This model is pretrained on the Flickr style dataset (containing 80 K images) for image style recognition task. It aims at the artistic aspect recognition of photographs and it is fine-tuned from another network which has been pretrained on the ImageNet dataset for object recognition task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYpotthT16SG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras import initializers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "import h5py\n",
        "\n",
        "def traverse_datasets(hdf_file):\n",
        "\n",
        "    def h5py_dataset_iterator(g, prefix=''):\n",
        "        for key in g.keys():\n",
        "            item = g[key]\n",
        "            path = f'{prefix}/{key}'\n",
        "            if isinstance(item, h5py.Dataset): # test for dataset\n",
        "                yield (path, item)\n",
        "            elif isinstance(item, h5py.Group): # test for group (go down)\n",
        "                yield from h5py_dataset_iterator(item, path)\n",
        "\n",
        "    with h5py.File(hdf_file, 'r') as f:\n",
        "        for path, _ in h5py_dataset_iterator(f):\n",
        "            yield path\n",
        "            \n",
        "weights = {}\n",
        "filename = '../../pretrained-models/flickr_style.h5'\n",
        "\n",
        "with h5py.File(filename, 'r') as f:\n",
        "    for dset in traverse_datasets(filename):\n",
        "        print('Path:', dset)\n",
        "        # print('Shape:', f[dset].shape)\n",
        "        # print('Data type:', f[dset].dtype)\n",
        "        weights[dset] = f[dset][:]\n",
        "\n",
        "conv1_bias = weights['/conv1/conv1/bias:0']\n",
        "conv1_kernel = weights['/conv1/conv1/kernel:0']\n",
        "conv2_bias = weights['/conv2/conv2/bias:0']\n",
        "conv2_kernel = weights['/conv2/conv2/kernel:0']\n",
        "conv3_bias = weights['/conv3/conv3/bias:0']\n",
        "conv3_kernel = weights['/conv3/conv3/kernel:0']\n",
        "conv4_bias = weights['/conv4/conv4/bias:0']\n",
        "conv4_kernel = weights['/conv4/conv4/kernel:0']\n",
        "conv5_bias = weights['/conv5/conv5/bias:0']\n",
        "conv5_kernel = weights['/conv5/conv5/kernel:0']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emHBQMSvb5XW",
        "colab_type": "text"
      },
      "source": [
        "The architecture that will be used for the CNN is CaffeNet. First, we define the LRN (Local Response Normalization) layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUHJvQtCxvSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers.core import Layer\n",
        "from keras import backend as K\n",
        "\n",
        "class LRN(Layer):\n",
        "    \n",
        "    def __init__(self, n=5, alpha=0.0001, beta=0.75, k=2, **kwargs):\n",
        "        self.n = n\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.k = k\n",
        "        super(LRN, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.shape = input_shape\n",
        "        super(LRN, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        if K.common.image_dim_ordering == \"th\":\n",
        "            _, f, r, c = self.shape\n",
        "        else:\n",
        "            _, r, c, f = self.shape\n",
        "        half_n = self.n // 2\n",
        "        squared = K.square(x)\n",
        "        pooled = K.pool2d(squared, (half_n, half_n), strides=(1, 1),\n",
        "                         padding=\"same\", pool_mode=\"avg\")\n",
        "        if K.common.image_dim_ordering == \"th\":\n",
        "            summed = K.sum(pooled, axis=1, keepdims=True)\n",
        "            averaged = (self.alpha / self.n) * K.repeat_elements(summed, f, axis=1)\n",
        "        else:\n",
        "            summed = K.sum(pooled, axis=3, keepdims=True)\n",
        "            averaged = (self.alpha / self.n) * K.repeat_elements(summed, f, axis=3)\n",
        "        denom = K.pow(self.k + averaged, self.beta)\n",
        "        return x / denom\n",
        "    \n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return input_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bseHCp0XcioH",
        "colab_type": "text"
      },
      "source": [
        "Construct the CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9DKuBfamcaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = 0.001 # weight decay\n",
        "\n",
        "input_shape = (192, 256, 3)\n",
        "\n",
        "im_data = layers.Input(shape=input_shape, dtype='float32', name='im_data')\n",
        "\n",
        "conv1 = layers.Conv2D(96, kernel_size=(11, 11), strides=(4, 4), name='conv1', \n",
        "                        activation='relu', input_shape=input_shape, \n",
        "                        kernel_regularizer=regularizers.l2(l))(im_data)\n",
        "\n",
        "pool1 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
        "norm1 = LRN(name=\"norm1\")(pool1)\n",
        "drop1 = layers.Dropout(0.1)(norm1)\n",
        "\n",
        "layer1_1 = layers.Lambda(lambda x: x[:, :, :, :48])(drop1)\n",
        "layer1_2 = layers.Lambda(lambda x: x[:, :, :, 48:])(drop1)\n",
        "\n",
        "conv2_1 = layers.Conv2D(128, kernel_size=(5, 5), strides=(1, 1),\n",
        "                        activation='relu',\n",
        "                        padding='same', \n",
        "                        name='conv2_1', \n",
        "                        kernel_regularizer=regularizers.l2(l))(layer1_1)\n",
        "\n",
        "conv2_2 = layers.Conv2D(128, kernel_size=(5, 5), strides=(1, 1),\n",
        "                        activation='relu',\n",
        "                        padding='same', \n",
        "                        name='conv2_2',\n",
        "                        kernel_regularizer=regularizers.l2(l))(layer1_2)\n",
        "\n",
        "conv2 = layers.Concatenate(name='conv_2')([conv2_1, conv2_2])\n",
        "\n",
        "pool2 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv2)\n",
        "norm2 = LRN(name=\"norm2\")(pool2)\n",
        "drop2 = layers.Dropout(0.1)(norm2)\n",
        "\n",
        "conv3 = layers.Conv2D(384, kernel_size=(3, 3), strides=(1, 1), activation='relu', \n",
        "                        name='conv3',\n",
        "                        padding='same',\n",
        "                        kernel_regularizer=regularizers.l2(l))(drop2)\n",
        "drop3 = layers.Dropout(0.1)(conv3)\n",
        "\n",
        "layer3_1 = layers.Lambda(lambda x: x[:, :, :, :192])(drop3)\n",
        "layer3_2 = layers.Lambda(lambda x: x[:, :, :, 192:])(drop3)\n",
        "\n",
        "conv4_1 = layers.Conv2D(192, kernel_size=(3, 3), strides=(1, 1),\n",
        "                        activation='relu', \n",
        "                        padding='same',\n",
        "                        name='conv4_1',\n",
        "                        kernel_regularizer=regularizers.l2(l))(layer3_1)\n",
        "\n",
        "conv4_2 = layers.Conv2D(192, kernel_size=(3, 3), strides=(1, 1),\n",
        "                        activation='relu', \n",
        "                        padding='same',\n",
        "                        name='conv4_2',\n",
        "                        kernel_regularizer=regularizers.l2(l))(layer3_2)\n",
        "\n",
        "conv4 = layers.Concatenate(name='conv_4')([conv4_1, conv4_2])\n",
        "\n",
        "layer4_1 = layers.Lambda(lambda x: x[:, :, :, :192])(conv4)\n",
        "layer4_2 = layers.Lambda(lambda x: x[:, :, :, 192:])(conv4)\n",
        "\n",
        "conv5_1 = layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1),\n",
        "                        activation='relu',\n",
        "                        padding='same', \n",
        "                        name='conv5_1',\n",
        "                        kernel_regularizer=regularizers.l2(l))(layer4_1)\n",
        "\n",
        "conv5_2 = layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1),\n",
        "                        activation='relu',\n",
        "                        padding='same', \n",
        "                        name='conv5_2',\n",
        "                        kernel_regularizer=regularizers.l2(l))(layer4_2)\n",
        "\n",
        "conv5 = layers.Concatenate(name='conv_5')([conv5_1, conv5_2])\n",
        "\n",
        "pool5 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv5)\n",
        "\n",
        "flat = layers.Flatten()(pool5)\n",
        "fc6 = layers.Dense(1024, activation='relu', name='fc6',\n",
        "                        kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
        "                        bias_initializer='zeros',\n",
        "                        kernel_regularizer=regularizers.l2(l))(flat)\n",
        "drop6 = layers.Dropout(0.5)(fc6)\n",
        "\n",
        "fc7 = layers.Dense(512, activation='relu', name='fc7', \n",
        "                        kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
        "                        bias_initializer='zeros',\n",
        "                        kernel_regularizer=regularizers.l2(l))(drop6)\n",
        "drop7 = layers.Dropout(0.5)(fc7)\n",
        "\n",
        "fc8 = layers.Dense(1, name='fc8',\n",
        "                        kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
        "                        bias_initializer='zeros')(drop7)\n",
        "\n",
        "model = models.Model(inputs=im_data, outputs=fc8)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWW_FBfacnmX",
        "colab_type": "text"
      },
      "source": [
        "Initialize the weights of the first 5 convolutional layers of the model with the weights of the pretrained model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuBkLvFF5CQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.get_layer('conv1').set_weights([conv1_kernel[:, :, :, :], conv1_bias[:]])\n",
        "model.get_layer('conv2_1').set_weights([conv2_kernel[:, :, :, :128], conv2_bias[:128]])\n",
        "model.get_layer('conv2_2').set_weights([conv2_kernel[:, :, :, 128:], conv2_bias[128:]])\n",
        "model.get_layer('conv3').set_weights([conv3_kernel[:, :, :, :], conv3_bias[:]])\n",
        "model.get_layer('conv4_1').set_weights([conv4_kernel[:, :, :, :192], conv4_bias[:192]])\n",
        "model.get_layer('conv4_2').set_weights([conv4_kernel[:, :, :, 192:], conv4_bias[192:]])\n",
        "model.get_layer('conv5_1').set_weights([conv5_kernel[:, :, :, :128], conv5_bias[:128]])\n",
        "model.get_layer('conv5_2').set_weights([conv5_kernel[:, :, :, 128:], conv5_bias[128:]])\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUGt7XDYcs8Z",
        "colab_type": "text"
      },
      "source": [
        "Define the loss function (Euclidean Distance Loss) and the metric (RMSE) that will be used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVch1nNr5Rg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        " \n",
        "def rmse(y_true, y_pred):\n",
        "\treturn K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
        "\n",
        "def euclidean_distance_loss(y_true, y_pred):\n",
        "    return 0.5 * K.mean(K.square(y_pred - y_true), axis=-1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVHYiqem9sgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)                                  \n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
        "val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHSX1b1Xc4k5",
        "colab_type": "text"
      },
      "source": [
        "Compile the model. Use RMSE to measure performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViI0HMCvHHs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 95\n",
        "decay = 1e-4 \n",
        "base_lr = 0.001\n",
        "\n",
        "sgd = optimizers.SGD(lr=base_lr, momentum=0.9, decay=decay, nesterov=True)\n",
        "model.compile(loss=euclidean_distance_loss , optimizer=sgd, metrics=[rmse])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxIiDcUMc9Sa",
        "colab_type": "text"
      },
      "source": [
        "Train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWKAbOhaIPAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit_generator(train_generator, \n",
        "                             steps_per_epoch = ntrain // batch_size,\n",
        "                             epochs = epochs, \n",
        "                             validation_data=val_generator,\n",
        "                             validation_steps=nval // batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ9uU7-PdFqU",
        "colab_type": "text"
      },
      "source": [
        "Display the learning curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMI1gcHnI6gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rmse = history.history[\"rmse\"]\n",
        "val_rmse = history.history[\"val_rmse\"]\n",
        "\n",
        "epochs_x = range(1, len(rmse) + 1)\n",
        "\n",
        "plt.plot(epochs_x, rmse, 'b', label='Training RMSE')\n",
        "plt.plot(epochs_x, val_rmse, 'r', label='Validation RMSE')\n",
        "\n",
        "plt.legend()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7GnqeMxdK0B",
        "colab_type": "text"
      },
      "source": [
        "Define a function that calculates Pearson correlation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYECQqvc6Gdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import stats\n",
        "\n",
        "def pearsonr_ci(x,y,alpha=0.05):\n",
        "    ''' calculate Pearson correlation along with the confidence interval using scipy and numpy\n",
        "    Parameters\n",
        "    ----------\n",
        "    x, y : iterable object such as a list or np.array\n",
        "      Input for correlation calculation\n",
        "    alpha : float\n",
        "      Significance level. 0.05 by default\n",
        "    Returns\n",
        "    -------\n",
        "    r : float\n",
        "      Pearson's correlation coefficient\n",
        "    pval : float\n",
        "      The corresponding p value\n",
        "    lo, hi : float\n",
        "      The lower and upper bound of confidence intervals\n",
        "    '''\n",
        "    N = len(x)\n",
        "    r, p = stats.pearsonr(x,y)\n",
        "    r_z = np.arctanh(r)\n",
        "    se = 1/np.sqrt(N-3)\n",
        "    z = stats.norm.ppf(1-alpha/2)\n",
        "    lo_z, hi_z = r_z-z*se, r_z+z*se\n",
        "    lo, hi = np.tanh((lo_z, hi_z))\n",
        "    return r, p, lo, hi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXH_a0lvdTTU",
        "colab_type": "text"
      },
      "source": [
        "Predict the scores for the images in the testing set. Display the predictions of the first 10 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnRLJ4jhLQWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "predictions = []\n",
        "\n",
        "X_val = X_val / 255.0\n",
        "for img in X_val:\n",
        "  img = img.reshape(1, 192, 256, 3)\n",
        "  pred = model.predict(img)\n",
        "  predictions.append(float(pred))\n",
        "\n",
        "predictions = np.array(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0UMecQMPl9d",
        "colab_type": "text"
      },
      "source": [
        "Display some websites of the test set and the predicted aesthetics score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQrmYKWzTxKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_ids = [87, 45, 49, 94, 14, 83] # test image IDs sorted in descending order according to the website's aesthetics level\n",
        "\n",
        "fig = plt.figure(figsize=(12, 16))\n",
        "i = 1\n",
        "for id in image_ids:\n",
        "  if 'english' in test_images[id]:\n",
        "    path = images_path + '/english_resized/' + test_images[id].rsplit('/', 1)[1]\n",
        "  else:\n",
        "    path = images_path + '/foreign_resized/' + test_images[id].rsplit('/', 1)[1]\n",
        "\n",
        "  plt.subplot(len(image_ids)/2, 2, i)\n",
        "  img = mping.imread(path)\n",
        "  plt.title('User average rating: ' + str(np.round(y_val[id],2)) + '\\nPredicted rating: ' + str(np.round(predictions[id],2)) + '\\n(' + chr(97+i-1) + ')', y=-0.25)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "    \n",
        "  i += 1\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6gPCKKKdW-F",
        "colab_type": "text"
      },
      "source": [
        "Create a scatterplot to check the relationship between ground truth and predicted scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r11x1AJ9I2VA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.polynomial.polynomial import polyfit\n",
        "b, m = polyfit(y_val, predictions, 1)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.scatter(y_val, predictions, c='c')\n",
        "plt.plot(y_val, b + m * y_val, '-', c='b')\n",
        "plt.xlabel('User ratings')\n",
        "plt.ylabel('Predicted ratings')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNgkCJm_da1i",
        "colab_type": "text"
      },
      "source": [
        "Calculate the Pearson correlation and the RMSE between ground truth and predicted scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGJrd24pKDUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "corr, p, lo, hi = pearsonr_ci(y_val, predictions)\n",
        "print('Pearsons correlation: r=%.2f, p=%.2e, CI=[%.2f, %.2f]' % (corr, p, lo, hi))\n",
        "rmse_test = sqrt(mean_squared_error(y_val, predictions))\n",
        "print('RMSE: %.3f' % rmse_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTsUydzKdeLz",
        "colab_type": "text"
      },
      "source": [
        "Plot the distribution of ground truth scores and the distribution of predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5U7G3ZqqjwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "fig = plt.figure()\n",
        "sns.set(color_codes=True)\n",
        "\n",
        "bins = np.linspace(1, 9, num=15)\n",
        "\n",
        "sns.distplot(y_val, bins=bins, label='User ratings')\n",
        "\n",
        "sns.distplot(predictions, bins=bins, label='Predicted ratings')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}