{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wce8xK3zgUS_",
        "colab_type": "text"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OORCOveHjVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r ../../requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4Xi_K0iJi-",
        "colab_type": "text"
      },
      "source": [
        "Import training and testing data.\n",
        "\n",
        "* {train, test}_images: contains the path of each image\n",
        "* {train, test}_scores: contains the user ratings of each image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hRyplkvJ_4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "data_folder = '../../datasets/rating-based-dataset/preprocess/'\n",
        "\n",
        "train_data_path = data_folder + 'train_list.csv'\n",
        "test_data_path = data_folder + 'test_list.csv'\n",
        "images_path = data_folder + 'resized'\n",
        "\n",
        "def get_scores(scores_path):\n",
        "\n",
        "    images = []\n",
        "    scores = []\n",
        "\n",
        "    with open(scores_path) as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "        line_count = 0\n",
        "\n",
        "        for row in csv_reader:\n",
        "            if line_count == 0:\n",
        "\n",
        "                line_count += 1\n",
        "            else:\n",
        "\n",
        "                scores.append(float(row[1]))\n",
        "                line_count += 1\n",
        "                image_name = row[0]\n",
        "\n",
        "                images.append(images_path + image_name)\n",
        "\n",
        "\n",
        "    return (images, scores)\n",
        "\n",
        "\n",
        "train_images, train_scores = get_scores(train_data_path)\n",
        "test_images, test_scores = get_scores(test_data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTpr09n0gkLB",
        "colab_type": "text"
      },
      "source": [
        "Shuffle the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FRsOM92KDV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# np.random.seed(2000)\n",
        "\n",
        "temp = list(zip(train_images, train_scores))\n",
        "random.shuffle(temp)\n",
        "\n",
        "train_images, train_scores = zip(*temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx7TZWWagn3T",
        "colab_type": "text"
      },
      "source": [
        "Display the first 3 images to make sure everything is ok. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnqW8Lv3KIGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.image as mping\n",
        "for ima in train_images[0:3]:\n",
        "    img = mping.imread(ima)\n",
        "    imgplot = plt.imshow(img)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2dZan3JivQM",
        "colab_type": "text"
      },
      "source": [
        "Convert data to numpy arrays and display shape to check everything is ok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU4JQdJtKO2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_filenames = np.array(train_images)\n",
        "y_train = np.array(train_scores)\n",
        "X_val_filenames = np.array(test_images)\n",
        "y_val = np.array(test_scores)\n",
        "\n",
        "ntrain = len(X_train_filenames)\n",
        "nval = len(X_val_filenames)\n",
        "\n",
        "print('Shape of X_train is: ', X_train_filenames.shape)\n",
        "print('Shape of X_val is: ', X_val_filenames.shape)\n",
        "print('Shape of y_train is: ', y_train.shape)\n",
        "print('Shape of y_val is: ', y_val.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkbtlDPcjEba",
        "colab_type": "text"
      },
      "source": [
        "Get the weights of the pretrained model. This model is pretrained on the Flickr style dataset (containing 80 K images) for image style recognition task. It aims at the artistic aspect recognition of photographs and it is fine-tuned from another network which has been pretrained on the ImageNet dataset for object recognition task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCYX1puvKfLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras import initializers\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "import h5py\n",
        "\n",
        "def traverse_datasets(hdf_file):\n",
        "\n",
        "    def h5py_dataset_iterator(g, prefix=''):\n",
        "        for key in g.keys():\n",
        "            item = g[key]\n",
        "            path = f'{prefix}/{key}'\n",
        "            if isinstance(item, h5py.Dataset): # test for dataset\n",
        "                yield (path, item)\n",
        "            elif isinstance(item, h5py.Group): # test for group (go down)\n",
        "                yield from h5py_dataset_iterator(item, path)\n",
        "\n",
        "    with h5py.File(hdf_file, 'r') as f:\n",
        "        for path, _ in h5py_dataset_iterator(f):\n",
        "            yield path\n",
        "            \n",
        "weights = {}\n",
        "\n",
        "filename = '../../pretrained-models/flickr_style.h5'\n",
        "\n",
        "with h5py.File(filename, 'r') as f:\n",
        "    for dset in traverse_datasets(filename):\n",
        "        # print('Shape:', f[dset].shape)\n",
        "        # print('Data type:', f[dset].dtype)\n",
        "        weights[dset] = f[dset][:]\n",
        "\n",
        "conv1_bias = weights['/conv1/conv1/bias:0']\n",
        "conv1_kernel = weights['/conv1/conv1/kernel:0']\n",
        "conv2_bias = weights['/conv2/conv2/bias:0']\n",
        "conv2_kernel = weights['/conv2/conv2/kernel:0']\n",
        "conv3_bias = weights['/conv3/conv3/bias:0']\n",
        "conv3_kernel = weights['/conv3/conv3/kernel:0']\n",
        "conv4_bias = weights['/conv4/conv4/bias:0']\n",
        "conv4_kernel = weights['/conv4/conv4/kernel:0']\n",
        "conv5_bias = weights['/conv5/conv5/bias:0']\n",
        "conv5_kernel = weights['/conv5/conv5/kernel:0']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM2TvYbDkURA",
        "colab_type": "text"
      },
      "source": [
        "The architecture that will be used for the CNN is AlexNet. First, we define the LRN (Local Response Normalization) layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR8sefrLKMRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers.core import Layer\n",
        "from keras import backend as K\n",
        "\n",
        "class LRN(Layer):\n",
        "    \n",
        "    def __init__(self, n=5, alpha=0.0001, beta=0.75, k=2, **kwargs):\n",
        "        self.n = n\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.k = k\n",
        "        super(LRN, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.shape = input_shape\n",
        "        super(LRN, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        if K.common.image_dim_ordering == \"th\":\n",
        "            _, f, r, c = self.shape\n",
        "        else:\n",
        "            _, r, c, f = self.shape\n",
        "        half_n = self.n // 2\n",
        "        squared = K.square(x)\n",
        "        pooled = K.pool2d(squared, (half_n, half_n), strides=(1, 1),\n",
        "                         padding=\"same\", pool_mode=\"avg\")\n",
        "        if K.common.image_dim_ordering == \"th\":\n",
        "            summed = K.sum(pooled, axis=1, keepdims=True)\n",
        "            averaged = (self.alpha / self.n) * K.repeat_elements(summed, f, axis=1)\n",
        "        else:\n",
        "            summed = K.sum(pooled, axis=3, keepdims=True)\n",
        "            averaged = (self.alpha / self.n) * K.repeat_elements(summed, f, axis=3)\n",
        "        denom = K.pow(self.k + averaged, self.beta)\n",
        "        return x / denom\n",
        "    \n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return input_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69SzXZzFlKrx",
        "colab_type": "text"
      },
      "source": [
        "Construct the CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lNeeMzdKyV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = 0.001 # weight decay\n",
        "\n",
        "input_shape = (192, 256, 3)\n",
        "\n",
        "im_data = layers.Input(shape=input_shape, dtype='float32', name='im_data')\n",
        "\n",
        "conv1 = layers.Conv2D(96, kernel_size=(11, 11), strides=(4, 4), name='conv1', \n",
        "                        activation='relu', input_shape=input_shape, \n",
        "                        kernel_regularizer=regularizers.l2(l))(im_data)\n",
        "\n",
        "pool1 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
        "norm1 = LRN(name=\"norm1\")(pool1)\n",
        "drop1 = layers.Dropout(0.1)(norm1)\n",
        "\n",
        "layer1_1 = layers.Lambda(lambda x: x[:, :, :, :48])(drop1)\n",
        "layer1_2 = layers.Lambda(lambda x: x[:, :, :, 48:])(drop1)\n",
        "\n",
        "conv2_1 = layers.Conv2D(128, kernel_size=(5, 5), strides=(1, 1),\n",
        "                          activation='relu',\n",
        "                          padding='same', \n",
        "                          name='conv2_1', \n",
        "                        kernel_regularizer=regularizers.l2(l))(layer1_1)\n",
        "\n",
        "conv2_2 = layers.Conv2D(128, kernel_size=(5, 5), strides=(1, 1),\n",
        "                 activation='relu',\n",
        "                 padding='same', \n",
        "                 name='conv2_2',\n",
        "                 kernel_regularizer=regularizers.l2(l))(layer1_2)\n",
        "\n",
        "conv2 = layers.Concatenate(name='conv_2')([conv2_1, conv2_2])\n",
        "\n",
        "pool2 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv2)\n",
        "norm2 = LRN(name=\"norm2\")(pool2)\n",
        "drop2 = layers.Dropout(0.1)(norm2)\n",
        "\n",
        "conv3 = layers.Conv2D(384, kernel_size=(3, 3), strides=(1, 1), activation='relu', name='conv3',\n",
        "                        padding='same',\n",
        "                        kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
        "                        bias_initializer='zeros',\n",
        "                        kernel_regularizer=regularizers.l2(l))(drop2)\n",
        "drop3 = layers.Dropout(0.1)(conv3)\n",
        "\n",
        "layer3_1 = layers.Lambda(lambda x: x[:, :, :, :192])(drop3)\n",
        "layer3_2 = layers.Lambda(lambda x: x[:, :, :, 192:])(drop3)\n",
        "\n",
        "conv4_1 = layers.Conv2D(192, kernel_size=(3, 3), strides=(1, 1),\n",
        "                 activation='relu', \n",
        "                 padding='same',\n",
        "                 name='conv4_1',\n",
        "                 kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=regularizers.l2(l))(layer3_1)\n",
        "\n",
        "conv4_2 = layers.Conv2D(192, kernel_size=(3, 3), strides=(1, 1),\n",
        "                 activation='relu', \n",
        "                 padding='same',\n",
        "                 name='conv4_2',\n",
        "                 kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=regularizers.l2(l))(layer3_2)\n",
        "\n",
        "conv4 = layers.Concatenate(name='conv_4')([conv4_1, conv4_2])\n",
        "\n",
        "layer4_1 = layers.Lambda(lambda x: x[:, :, :, :192])(conv4)\n",
        "layer4_2 = layers.Lambda(lambda x: x[:, :, :, 192:])(conv4)\n",
        "\n",
        "conv5_1 = layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1),\n",
        "                 activation='relu',\n",
        "                 padding='same', \n",
        "                 name='conv5_1',\n",
        "                 kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=regularizers.l2(l))(layer4_1)\n",
        "\n",
        "conv5_2 = layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1),\n",
        "                 activation='relu',\n",
        "                 padding='same', \n",
        "                 name='conv5_2',\n",
        "                 kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=regularizers.l2(l))(layer4_2)\n",
        "\n",
        "conv5 = layers.Concatenate(name='conv_5')([conv5_1, conv5_2])\n",
        "\n",
        "pool5 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv5)\n",
        "\n",
        "flat = layers.Flatten()(pool5)\n",
        "fc6 = layers.Dense(1024, activation='relu', \n",
        "                        kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
        "                        bias_initializer='zeros',\n",
        "                       kernel_regularizer=regularizers.l2(l))(flat)\n",
        "drop6 = layers.Dropout(0.5)(fc6)\n",
        "\n",
        "fc7 = layers.Dense(512, activation='relu',\n",
        "                        kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
        "                        bias_initializer='zeros',\n",
        "                       kernel_regularizer=regularizers.l2(l))(drop6)\n",
        "drop7 = layers.Dropout(0.5)(fc7)\n",
        "\n",
        "fc8 = layers.Dense(1,\n",
        "                  kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
        "                  bias_initializer='zeros')(drop7)\n",
        "\n",
        "model = models.Model(inputs=im_data, outputs=fc8)\n",
        "\n",
        "# model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6EO3DlrSr1Q",
        "colab_type": "text"
      },
      "source": [
        "Initialize the weights of the first 2 convolutional layers of the model with the weights of the pretrained model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9qSrvFsQTQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.get_layer('conv1').set_weights([conv1_kernel[:, :, :, :], conv1_bias[:]])\n",
        "model.get_layer('conv2_1').set_weights([conv2_kernel[:, :, :, :128], conv2_bias[:128]])\n",
        "model.get_layer('conv2_2').set_weights([conv2_kernel[:, :, :, 128:], conv2_bias[128:]])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BK-EQ6dlXgR",
        "colab_type": "text"
      },
      "source": [
        "Define the loss function (Euclidean Distance Loss) and the metric (RMSE) that will be used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0B_jd9FO9wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        " \n",
        "def rmse(y_true, y_pred):\n",
        "\treturn K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
        "\n",
        "def euclidean_distance_loss(y_true, y_pred):\n",
        "    return 0.5 * K.mean(K.square(y_pred - y_true), axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE5BHHnzRP11",
        "colab_type": "text"
      },
      "source": [
        "Define a generator to load the training data in batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx6l_eRWKzvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import utils\n",
        "import cv2\n",
        "\n",
        "class My_Custom_Generator(utils.Sequence) :\n",
        "  \n",
        "  def __init__(self, image_filenames, labels, batch_size) :\n",
        "    self.image_filenames = image_filenames\n",
        "    self.labels = labels\n",
        "    self.batch_size = batch_size\n",
        "    \n",
        "    \n",
        "  def __len__(self) :\n",
        "    return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(np.int)\n",
        "  \n",
        "  \n",
        "  def __getitem__(self, idx) :\n",
        "    batch_x = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "      \n",
        "\n",
        "    return np.array([\n",
        "            cv2.imread(str(file_name), cv2.IMREAD_COLOR)\n",
        "               for file_name in batch_x]) / 255.0, np.array(batch_y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAfUN_FSO3mH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_generator = My_Custom_Generator(X_train_filenames, y_train, batch_size)\n",
        "val_generator = My_Custom_Generator(X_val_filenames, y_val, 98)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoN417QIQFXe",
        "colab_type": "text"
      },
      "source": [
        "Compile the model. Use RMSE to measure performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeH_erj3PLZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 4\n",
        "decay = 1e-2\n",
        "base_lr = 0.001\n",
        "\n",
        "sgd = optimizers.SGD(lr=base_lr, momentum=0.9, decay=decay, nesterov=True)\n",
        "model.compile(loss=euclidean_distance_loss , optimizer=sgd, metrics=[rmse])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TljPbLZ5QPs8",
        "colab_type": "text"
      },
      "source": [
        "Train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxJCv3XuPOLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit_generator(train_generator, \n",
        "                             steps_per_epoch = ntrain // batch_size,\n",
        "                             epochs = epochs, \n",
        "                             validation_data=val_generator,\n",
        "                             validation_steps=nval // 98)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk5j6SFRKt5B",
        "colab_type": "text"
      },
      "source": [
        "Display the learning curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHyO0Z31P1FP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rmse = history.history[\"rmse\"]\n",
        "val_rmse = history.history[\"val_rmse\"]\n",
        "\n",
        "epochs_x = range(1, len(rmse) + 1)\n",
        "\n",
        "plt.plot(epochs_x, rmse, 'b', label='Training RMSE')\n",
        "plt.plot(epochs_x, val_rmse, 'r', label='Validation RMSE')\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySOlU9fZQf7U",
        "colab_type": "text"
      },
      "source": [
        "Define a function that calculates Pearson correlation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKx891j4KLSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import stats\n",
        "\n",
        "def pearsonr_ci(x,y,alpha=0.05):\n",
        "    ''' calculate Pearson correlation along with the confidence interval using scipy and numpy\n",
        "    Parameters\n",
        "    ----------\n",
        "    x, y : iterable object such as a list or np.array\n",
        "      Input for correlation calculation\n",
        "    alpha : float\n",
        "      Significance level. 0.05 by default\n",
        "    Returns\n",
        "    -------\n",
        "    r : float\n",
        "      Pearson's correlation coefficient\n",
        "    pval : float\n",
        "      The corresponding p value\n",
        "    lo, hi : float\n",
        "      The lower and upper bound of confidence intervals\n",
        "    '''\n",
        "    N = len(x)\n",
        "    r, p = stats.pearsonr(x,y)\n",
        "    r_z = np.arctanh(r)\n",
        "    se = 1/np.sqrt(N-3)\n",
        "    z = stats.norm.ppf(1-alpha/2)\n",
        "    lo_z, hi_z = r_z-z*se, r_z+z*se\n",
        "    lo, hi = np.tanh((lo_z, hi_z))\n",
        "    return r, p, lo, hi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFu-fScQKk7H",
        "colab_type": "text"
      },
      "source": [
        "Load the images of the testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlriePUuP3gB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "def read_and_process_images(list_of_images):\n",
        "  X = []\n",
        "  \n",
        "  for image in list_of_images:\n",
        "    X.append(np.array(cv2.imread(image, cv2.IMREAD_COLOR)) / 255.0)\n",
        "\n",
        "  \n",
        "  return X\n",
        "\n",
        "X_val = np.array(read_and_process_images(test_images))\n",
        "y_val = np.array(test_scores)\n",
        "\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5HPPhTNIFJn",
        "colab_type": "text"
      },
      "source": [
        "Predict the scores for the images in the testing set. Display the predictions of the first 10 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xcWQ115P72a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = []\n",
        "\n",
        "for img in X_val:\n",
        "  img = img.reshape(1, 192, 256, 3)\n",
        "  pred = model.predict(img)\n",
        "  predictions.append(float(pred))\n",
        "\n",
        "predictions = np.array(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbTr3uDSz2BP",
        "colab_type": "text"
      },
      "source": [
        "Display some websites of the test set and the predicted aesthetics score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caw2MCm_z4Y6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_ids = [87, 45, 49, 94, 14, 83] # test image IDs sorted in descending order according to the website's aesthetics level\n",
        "\n",
        "fig = plt.figure(figsize=(12, 16))\n",
        "i = 1\n",
        "for id in image_ids:\n",
        "  if 'english' in test_images[id]:\n",
        "    path = images_path + '/english_resized/' + test_images[id].rsplit('/', 1)[1]\n",
        "  else:\n",
        "    path = images_path + '/foreign_resized/' + test_images[id].rsplit('/', 1)[1]\n",
        "\n",
        "  plt.subplot(len(image_ids)/2, 2, i)\n",
        "  img = mping.imread(path)\n",
        "  plt.title('User average rating: ' + str(np.round(y_val[id],2)) + '\\nPredicted rating: ' + str(np.round(predictions[id],2)) + '\\n(' + chr(97+i-1) + ')', y=-0.25)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "    \n",
        "  i += 1\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyRJTbIOHAeq",
        "colab_type": "text"
      },
      "source": [
        "Create a scatterplot to check the relationship between ground truth and predicted scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLHDCu5oXMo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.polynomial.polynomial import polyfit\n",
        "b, m = polyfit(y_val, predictions, 1)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.scatter(y_val, predictions, c='c')\n",
        "plt.plot(y_val, b + m * y_val, '-', c='b')\n",
        "plt.xlabel('User ratings')\n",
        "plt.ylabel('Predicted ratings')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LReGzPZAGxiC",
        "colab_type": "text"
      },
      "source": [
        "Calculate the Pearson correlation and the RMSE between ground truth and predicted scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neMkG__hXNZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "corr, p, lo, hi = pearsonr_ci(y_val, predictions)\n",
        "print('Pearsons correlation: r=%.2f, p=%.2e, CI=[%.2f, %.2f]' % (corr, p, lo, hi))\n",
        "rmse_test = sqrt(mean_squared_error(y_val, predictions))\n",
        "print('RMSE: %.3f' % rmse_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v1gctTDGotc",
        "colab_type": "text"
      },
      "source": [
        "Plot the distribution of ground truth scores and the distribution of predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li70DYFtXVwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "fig = plt.figure()\n",
        "sns.set(color_codes=True)\n",
        "\n",
        "bins = np.linspace(1, 9, num=15)\n",
        "\n",
        "sns.distplot(y_val, bins=bins, label='User ratings')\n",
        "\n",
        "sns.distplot(predictions, bins=bins, label='Predicted ratings')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}